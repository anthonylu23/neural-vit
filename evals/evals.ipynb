{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Run Plots\n",
    "\n",
    "Plots loss, AUC, and accuracy curves for each run listed in `run_details.json`.\n",
    "Best values are highlighted with markers. Baseline results are loaded from `baseline_results.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def resolve_path(path_str: str) -> Path:\n",
    "    \"\"\"Resolve path from multiple candidate locations.\"\"\"\n",
    "    path = Path(path_str)\n",
    "    candidates = [\n",
    "        path,\n",
    "        Path(path.name),\n",
    "        Path('evals') / path.name,\n",
    "        Path('..') / path,\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return path\n",
    "\n",
    "\n",
    "def generate_run_name(run: Dict[str, Any]) -> str:\n",
    "    \"\"\"Generate a unique/differentiator name from run parameters or run_id.\"\"\"\n",
    "    params = run.get('params')\n",
    "    run_id = run.get('run_id', 'unknown')\n",
    "    \n",
    "    # Extract timestamp from run_id (e.g., temporal-vit-20260106-044352 -> 01/06-0443)\n",
    "    parts = run_id.split('-')\n",
    "    if len(parts) >= 4:\n",
    "        date_part = parts[-2]  # e.g., 20260106\n",
    "        time_part = parts[-1]  # e.g., 044352\n",
    "        short_ts = f\"{date_part[4:6]}/{date_part[6:8]}-{time_part[:4]}\"\n",
    "    else:\n",
    "        short_ts = run_id[-12:] if len(run_id) > 12 else run_id\n",
    "    \n",
    "    if params and isinstance(params, dict):\n",
    "        # Build name from key parameters\n",
    "        name_parts = []\n",
    "        \n",
    "        # Key differentiating parameters\n",
    "        param_abbrevs = {\n",
    "            'n_trials': 'tr',\n",
    "            'embed_dim': 'dim',\n",
    "            'n_layers': 'L',\n",
    "            'n_heads': 'H',\n",
    "            'dropout': 'do',\n",
    "            'drop_path': 'dp',\n",
    "            'lr': 'lr',\n",
    "            'weight_decay': 'wd',\n",
    "            'label_smoothing': 'ls',\n",
    "            'batch_size': 'bs',\n",
    "        }\n",
    "        \n",
    "        for key, abbrev in param_abbrevs.items():\n",
    "            if key in params:\n",
    "                val = params[key]\n",
    "                if isinstance(val, float):\n",
    "                    if val < 0.01:\n",
    "                        val_str = f\"{val:.0e}\"\n",
    "                    else:\n",
    "                        val_str = f\"{val:.2g}\"\n",
    "                else:\n",
    "                    val_str = str(val)\n",
    "                name_parts.append(f\"{abbrev}{val_str}\")\n",
    "        \n",
    "        if name_parts:\n",
    "            return f\"{short_ts} ({', '.join(name_parts[:4])})\"\n",
    "    \n",
    "    return short_ts\n",
    "\n",
    "\n",
    "def extract_metrics_df(run: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Extract metrics from run into a DataFrame.\"\"\"\n",
    "    metrics = run.get('metrics', [])\n",
    "    if not metrics:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(metrics)\n",
    "    # Filter to training epochs only (exclude test-only rows)\n",
    "    if 'val/loss' in df.columns:\n",
    "        df = df.dropna(subset=['val/loss'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_baselines(path_str: str = 'evals/baseline_results.json') -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load baseline results from JSON file.\"\"\"\n",
    "    path = resolve_path(path_str)\n",
    "    if not path.exists():\n",
    "        print(f\"Baseline results not found at {path}. Run collect_baseline_results.py first.\")\n",
    "        return []\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as handle:\n",
    "        payload = json.load(handle)\n",
    "    return payload.get('baselines', [])\n",
    "\n",
    "\n",
    "# Load run details\n",
    "run_details_path = resolve_path('evals/run_details.json')\n",
    "if not run_details_path.exists():\n",
    "    run_details_path = resolve_path('run_details.json')\n",
    "\n",
    "with open(run_details_path, 'r', encoding='utf-8') as handle:\n",
    "    payload = json.load(handle)\n",
    "\n",
    "runs = payload.get('runs', [])\n",
    "if not runs:\n",
    "    raise ValueError('No runs found in run_details.json')\n",
    "\n",
    "baselines = load_baselines()\n",
    "\n",
    "print(f\"Found {len(runs)} runs\")\n",
    "print(f\"Found {len(baselines)} baseline results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HP tuning run details (optional)\n",
    "hptune_details_path = resolve_path('evals/hptune_run_details.json')\n",
    "if not hptune_details_path.exists():\n",
    "    hptune_details_path = resolve_path('hptune_run_details.json')\n",
    "\n",
    "hptune_runs = []\n",
    "if hptune_details_path.exists():\n",
    "    with open(hptune_details_path, 'r', encoding='utf-8') as handle:\n",
    "        hptune_payload = json.load(handle)\n",
    "    hptune_runs = hptune_payload.get('runs', [])\n",
    "    print(f\"Found {len(hptune_runs)} HP tuning runs\")\n",
    "else:\n",
    "    print(f\"HP tuning details not found at {hptune_details_path}\")\n",
    "\n",
    "def summarize_hptune_runs(runs):\n",
    "    rows = []\n",
    "    for run in runs:\n",
    "        summary = run.get('summary', {})\n",
    "        run_id = run.get('run_id') or 'unknown'\n",
    "        val_auc = summary.get('best_val_auc')\n",
    "        test_auc = summary.get('last_test_auc')\n",
    "        if val_auc is None and test_auc is None:\n",
    "            continue\n",
    "        rows.append({\n",
    "            'run_id': run_id,\n",
    "            'val_auc': val_auc,\n",
    "            'test_auc': test_auc,\n",
    "            'summary': summary,\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "hptune_data = summarize_hptune_runs(hptune_runs)\n",
    "print(f\"HP runs with summary metrics: {len(hptune_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for each run\n",
    "run_data = []\n",
    "for run in runs:\n",
    "    df = extract_metrics_df(run)\n",
    "    if df.empty:\n",
    "        continue\n",
    "    \n",
    "    name = generate_run_name(run)\n",
    "    run_data.append({\n",
    "        'name': name,\n",
    "        'run_id': run.get('run_id'),\n",
    "        'df': df,\n",
    "        'summary': run.get('summary', {}),\n",
    "    })\n",
    "\n",
    "print(f\"Runs with metrics: {len(run_data)}\")\n",
    "for rd in run_data:\n",
    "    print(f\"  - {rd['name']}: {len(rd['df'])} epochs\")\n",
    "\n",
    "print('\\nBaselines:')\n",
    "for bl in baselines:\n",
    "    test_auc = bl.get('test', {}).get('auc')\n",
    "    test_acc = bl.get('test', {}).get('acc')\n",
    "    auc_str = f\"{test_auc:.4f}\" if isinstance(test_auc, float) else 'N/A'\n",
    "    acc_str = f\"{test_acc:.4f}\" if isinstance(test_acc, float) else 'N/A'\n",
    "    print(f\"  - {bl['name']}: test_auc={auc_str}, test_acc={acc_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Loss over epochs\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "for rd in run_data:\n",
    "    df = rd['df']\n",
    "    name = rd['name']\n",
    "    \n",
    "    if 'val/loss' not in df.columns or 'step' not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    epochs = df['step'].values\n",
    "    val_loss = df['val/loss'].values\n",
    "    \n",
    "    # Plot the line\n",
    "    line, = ax.plot(epochs, val_loss, marker='o', markersize=4, label=name)\n",
    "    \n",
    "    # Highlight best (minimum) loss\n",
    "    best_idx = val_loss.argmin()\n",
    "    best_epoch = epochs[best_idx]\n",
    "    best_val = val_loss[best_idx]\n",
    "    ax.scatter([best_epoch], [best_val], s=150, c=line.get_color(), \n",
    "               marker='*', edgecolors='black', linewidths=1, zorder=5)\n",
    "    ax.annotate(f'{best_val:.3f}', (best_epoch, best_val), \n",
    "                textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
    "\n",
    "ax.set_title('Validation Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Validation AUC over epochs\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "for rd in run_data:\n",
    "    df = rd['df']\n",
    "    name = rd['name']\n",
    "    \n",
    "    if 'val/auc' not in df.columns or 'step' not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    epochs = df['step'].values\n",
    "    val_auc = df['val/auc'].values\n",
    "    \n",
    "    # Plot the line\n",
    "    line, = ax.plot(epochs, val_auc, marker='o', markersize=4, label=name)\n",
    "    \n",
    "    # Highlight best (maximum) AUC\n",
    "    best_idx = val_auc.argmax()\n",
    "    best_epoch = epochs[best_idx]\n",
    "    best_val = val_auc[best_idx]\n",
    "    ax.scatter([best_epoch], [best_val], s=150, c=line.get_color(), \n",
    "               marker='*', edgecolors='black', linewidths=1, zorder=5)\n",
    "    ax.annotate(f'{best_val:.4f}', (best_epoch, best_val), \n",
    "                textcoords='offset points', xytext=(5, -10), fontsize=8)\n",
    "\n",
    "ax.set_title('Validation AUC over Epochs', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('AUC', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Validation Accuracy over epochs\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "for rd in run_data:\n",
    "    df = rd['df']\n",
    "    name = rd['name']\n",
    "    \n",
    "    if 'val/acc' not in df.columns or 'step' not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    epochs = df['step'].values\n",
    "    val_acc = df['val/acc'].values\n",
    "    \n",
    "    # Plot the line\n",
    "    line, = ax.plot(epochs, val_acc, marker='o', markersize=4, label=name)\n",
    "    \n",
    "    # Highlight best (maximum) accuracy\n",
    "    best_idx = val_acc.argmax()\n",
    "    best_epoch = epochs[best_idx]\n",
    "    best_val = val_acc[best_idx]\n",
    "    ax.scatter([best_epoch], [best_val], s=150, c=line.get_color(), \n",
    "               marker='*', edgecolors='black', linewidths=1, zorder=5)\n",
    "    ax.annotate(f'{best_val:.3f}', (best_epoch, best_val), \n",
    "                textcoords='offset points', xytext=(5, -10), fontsize=8)\n",
    "\n",
    "ax.set_title('Validation Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Test AUC comparison (ViT vs baselines)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "model_names = []\n",
    "test_aucs = []\n",
    "colors = []\n",
    "\n",
    "for rd in run_data:\n",
    "    test_auc = rd['summary'].get('last_test_auc')\n",
    "    if isinstance(test_auc, float):\n",
    "        model_names.append(f\"ViT: {rd['name']}\")\n",
    "        test_aucs.append(test_auc)\n",
    "        colors.append('steelblue')\n",
    "\n",
    "baseline_color_map = {\n",
    "    'log_reg': 'coral',\n",
    "    'logistic_regression': 'coral',\n",
    "    'xgboost': 'forestgreen',\n",
    "    'random_forest': 'purple',\n",
    "}\n",
    "for bl in baselines:\n",
    "    test_auc = bl.get('test', {}).get('auc')\n",
    "    if isinstance(test_auc, float):\n",
    "        model_names.append(bl['name'])\n",
    "        test_aucs.append(test_auc)\n",
    "        colors.append(baseline_color_map.get(bl.get('model_type', ''), 'gray'))\n",
    "\n",
    "if not test_aucs:\n",
    "    print('No test AUC values available to plot.')\n",
    "else:\n",
    "    sorted_indices = sorted(range(len(test_aucs)), key=lambda i: test_aucs[i], reverse=True)\n",
    "    model_names = [model_names[i] for i in sorted_indices]\n",
    "    test_aucs = [test_aucs[i] for i in sorted_indices]\n",
    "    colors = [colors[i] for i in sorted_indices]\n",
    "\n",
    "    bars = ax.barh(range(len(model_names)), test_aucs, color=colors, edgecolor='black', linewidth=0.5)\n",
    "    for bar, auc in zip(bars, test_aucs):\n",
    "        ax.text(auc + 0.005, bar.get_y() + bar.get_height() / 2, f'{auc:.4f}',\n",
    "                va='center', ha='left', fontsize=9)\n",
    "\n",
    "    ax.set_yticks(range(len(model_names)))\n",
    "    ax.set_yticklabels(model_names, fontsize=10)\n",
    "    ax.set_xlabel('Test AUC', fontsize=12)\n",
    "    ax.set_title('Test AUC Comparison: ViT vs Baselines', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim(0, max(test_aucs) * 1.1)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='steelblue', edgecolor='black', label='ViT'),\n",
    "        Patch(facecolor='coral', edgecolor='black', label='Logistic Regression'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5: Test Accuracy comparison (ViT vs baselines)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "model_names = []\n",
    "test_accs = []\n",
    "colors = []\n",
    "\n",
    "for rd in run_data:\n",
    "    test_acc = rd['summary'].get('last_test_acc')\n",
    "    if isinstance(test_acc, float):\n",
    "        model_names.append(f\"ViT: {rd['name']}\")\n",
    "        test_accs.append(test_acc)\n",
    "        colors.append('steelblue')\n",
    "\n",
    "baseline_color_map = {\n",
    "    'log_reg': 'coral',\n",
    "    'logistic_regression': 'coral',\n",
    "    'xgboost': 'forestgreen',\n",
    "    'random_forest': 'purple',\n",
    "}\n",
    "for bl in baselines:\n",
    "    test_acc = bl.get('test', {}).get('acc')\n",
    "    if isinstance(test_acc, float):\n",
    "        model_names.append(bl['name'])\n",
    "        test_accs.append(test_acc)\n",
    "        colors.append(baseline_color_map.get(bl.get('model_type', ''), 'gray'))\n",
    "\n",
    "if not test_accs:\n",
    "    print('No test accuracy values available to plot.')\n",
    "else:\n",
    "    sorted_indices = sorted(range(len(test_accs)), key=lambda i: test_accs[i], reverse=True)\n",
    "    model_names = [model_names[i] for i in sorted_indices]\n",
    "    test_accs = [test_accs[i] for i in sorted_indices]\n",
    "    colors = [colors[i] for i in sorted_indices]\n",
    "\n",
    "    bars = ax.barh(range(len(model_names)), test_accs, color=colors, edgecolor='black', linewidth=0.5)\n",
    "    for bar, acc in zip(bars, test_accs):\n",
    "        ax.text(acc + 0.005, bar.get_y() + bar.get_height() / 2, f'{acc:.4f}',\n",
    "                va='center', ha='left', fontsize=9)\n",
    "\n",
    "    ax.set_yticks(range(len(model_names)))\n",
    "    ax.set_yticklabels(model_names, fontsize=10)\n",
    "    ax.set_xlabel('Test Accuracy', fontsize=12)\n",
    "    ax.set_title('Test Accuracy Comparison: ViT vs Baselines', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim(0, max(test_accs) * 1.1)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='steelblue', edgecolor='black', label='ViT'),\n",
    "        Patch(facecolor='coral', edgecolor='black', label='Logistic Regression'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table with baselines\n",
    "summary_data = []\n",
    "\n",
    "for rd in run_data:\n",
    "    df = rd['df']\n",
    "    summary = rd['summary']\n",
    "    \n",
    "    row = {\n",
    "        'Model': 'ViT',\n",
    "        'Run': rd['name'],\n",
    "        'Epochs': len(df),\n",
    "        'Best Val AUC': summary.get('best_val_auc'),\n",
    "        'Best AUC Epoch': summary.get('best_val_auc_step'),\n",
    "        'Test AUC': summary.get('last_test_auc'),\n",
    "        'Test Acc': summary.get('last_test_acc'),\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "\n",
    "for bl in baselines:\n",
    "    row = {\n",
    "        'Model': bl.get('model_type', 'baseline'),\n",
    "        'Run': bl['name'],\n",
    "        'Epochs': '-',\n",
    "        'Best Val AUC': bl.get('val', {}).get('auc'),\n",
    "        'Best AUC Epoch': '-',\n",
    "        'Test AUC': bl.get('test', {}).get('auc'),\n",
    "        'Test Acc': bl.get('test', {}).get('acc'),\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df['_sort_key'] = summary_df['Test AUC'].apply(lambda x: x if isinstance(x, float) else -1)\n",
    "summary_df = summary_df.sort_values('_sort_key', ascending=False).drop(columns=['_sort_key'])\n",
    "\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 6: HP tuning trial AUCs (validation vs test)\n",
    "if not hptune_data:\n",
    "    print('No HP tuning runs to plot.')\n",
    "else:\n",
    "    import numpy as np\n",
    "    import math\n",
    "\n",
    "    def short_run_id(run_id: str) -> str:\n",
    "        if not run_id:\n",
    "            return 'unknown'\n",
    "        return run_id[-8:] if len(run_id) > 8 else run_id\n",
    "\n",
    "    def add_value_labels(bars, values, y_pad=0.01):\n",
    "        for bar, val in zip(bars, values):\n",
    "            try:\n",
    "                val_f = float(val)\n",
    "            except (TypeError, ValueError):\n",
    "                continue\n",
    "            if math.isnan(val_f):\n",
    "                continue\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2,\n",
    "                bar.get_height() + y_pad,\n",
    "                f\"{val_f:.3f}\",\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=8,\n",
    "                rotation=0,\n",
    "                color='black',\n",
    "                bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=1.2),\n",
    "                clip_on=False,\n",
    "            )\n",
    "\n",
    "    labels = [short_run_id(row['run_id']) for row in hptune_data]\n",
    "    val_aucs = [row.get('val_auc') for row in hptune_data]\n",
    "    test_aucs = [row.get('test_auc') for row in hptune_data]\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.38\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(max(10, len(labels) * 0.55), 6))\n",
    "    val_bars = ax.bar(x - width / 2, val_aucs, width, label='Val AUC', color='steelblue')\n",
    "    test_bars = ax.bar(x + width / 2, test_aucs, width, label='Test AUC', color='darkorange')\n",
    "\n",
    "    valid_vals = [v for v in (val_aucs + test_aucs) if isinstance(v, (int, float)) and not math.isnan(v)]\n",
    "    y_max = max(valid_vals) if valid_vals else 1.0\n",
    "    ax.set_ylim(0, min(1.2, y_max + 0.08))\n",
    "\n",
    "    add_value_labels(val_bars, val_aucs)\n",
    "    add_value_labels(test_bars, test_aucs)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_ylabel('AUC')\n",
    "    ax.set_title('HP Tuning Trials: Validation vs Test AUC')\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 7: HP tuning gap (val - test) and scatter\n",
    "if not hptune_data:\n",
    "    print('No HP tuning runs to plot.')\n",
    "else:\n",
    "    import numpy as np\n",
    "    import math\n",
    "\n",
    "    def short_run_id(run_id: str) -> str:\n",
    "        if not run_id:\n",
    "            return 'unknown'\n",
    "        return run_id[-8:] if len(run_id) > 8 else run_id\n",
    "\n",
    "    labels = [short_run_id(row['run_id']) for row in hptune_data]\n",
    "    val_aucs = [row.get('val_auc') for row in hptune_data]\n",
    "    test_aucs = [row.get('test_auc') for row in hptune_data]\n",
    "\n",
    "    rows = []\n",
    "    for label, val, test in zip(labels, val_aucs, test_aucs):\n",
    "        if not isinstance(val, (int, float)) or not isinstance(test, (int, float)):\n",
    "            continue\n",
    "        if math.isnan(val) or math.isnan(test):\n",
    "            continue\n",
    "        rows.append((label, float(val), float(test)))\n",
    "\n",
    "    if not rows:\n",
    "        print('No valid HP runs with val/test AUC to plot.')\n",
    "    else:\n",
    "        labels_f, val_f, test_f = zip(*rows)\n",
    "        gaps = [v - t for v, t in zip(val_f, test_f)]\n",
    "\n",
    "        x = np.arange(len(labels_f))\n",
    "        fig, ax = plt.subplots(figsize=(max(10, len(labels_f) * 0.55), 5))\n",
    "        ax.bar(x, gaps, color='slategray')\n",
    "        ax.axhline(0, color='black', linewidth=0.8)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels_f, rotation=45, ha='right', fontsize=8)\n",
    "        ax.set_ylabel('Val AUC - Test AUC')\n",
    "        ax.set_title('HP Tuning Trials: Generalization Gap')\n",
    "        ax.grid(True, axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.scatter(val_f, test_f, color='teal', alpha=0.8)\n",
    "        ax.plot([0, 1], [0, 1], '--', color='gray')\n",
    "        ax.set_xlim(0, 1.0)\n",
    "        ax.set_ylim(0, 1.0)\n",
    "        ax.set_xlabel('Validation AUC')\n",
    "        ax.set_ylabel('Test AUC')\n",
    "        ax.set_title('HP Tuning Trials: Val vs Test AUC')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 8: HP tuning validation AUC over epochs\n",
    "if not hptune_runs:\n",
    "    print('No HP tuning runs to plot.')\n",
    "else:\n",
    "    import math\n",
    "\n",
    "    def short_run_id(run_id: str) -> str:\n",
    "        if not run_id:\n",
    "            return 'unknown'\n",
    "        return run_id[-8:] if len(run_id) > 8 else run_id\n",
    "\n",
    "    hp_run_data = []\n",
    "    for run in hptune_runs:\n",
    "        df = extract_metrics_df(run)\n",
    "        if df.empty or 'val/auc' not in df.columns or 'step' not in df.columns:\n",
    "            continue\n",
    "        hp_run_data.append({\n",
    "            'run_id': run.get('run_id') or 'unknown',\n",
    "            'df': df,\n",
    "        })\n",
    "\n",
    "    if not hp_run_data:\n",
    "        print('No HP tuning runs with per-epoch val AUC.')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        for rd in hp_run_data:\n",
    "            df = rd['df']\n",
    "            epochs = df['step'].values\n",
    "            val_auc = df['val/auc'].values\n",
    "            ax.plot(epochs, val_auc, marker='o', markersize=3, label=short_run_id(rd['run_id']))\n",
    "\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Val AUC')\n",
    "        ax.set_title('HP Tuning: Validation AUC over Epochs')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "        plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 9: HP tuning validation loss over epochs\n",
    "if not hptune_runs:\n",
    "    print('No HP tuning runs to plot.')\n",
    "else:\n",
    "    hp_run_data = []\n",
    "    for run in hptune_runs:\n",
    "        df = extract_metrics_df(run)\n",
    "        if df.empty or 'val/loss' not in df.columns or 'step' not in df.columns:\n",
    "            continue\n",
    "        hp_run_data.append({\n",
    "            'run_id': run.get('run_id') or 'unknown',\n",
    "            'df': df,\n",
    "        })\n",
    "\n",
    "    if not hp_run_data:\n",
    "        print('No HP tuning runs with per-epoch val loss.')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        for rd in hp_run_data:\n",
    "            df = rd['df']\n",
    "            epochs = df['step'].values\n",
    "            val_loss = df['val/loss'].values\n",
    "            ax.plot(epochs, val_loss, marker='o', markersize=3, label=short_run_id(rd['run_id']))\n",
    "\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Val Loss')\n",
    "        ax.set_title('HP Tuning: Validation Loss over Epochs')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "        plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}