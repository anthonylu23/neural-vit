{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Eval Run Plots\n",
        "\n",
        "Plots loss, AUC, and accuracy curves for each run listed in `run_details.json`.\n",
        "Best values are highlighted with markers. Baseline results are loaded from `baseline_results.json`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def resolve_path(path_str: str) -> Path:\n",
        "    \"\"\"Resolve path from multiple candidate locations.\"\"\"\n",
        "    path = Path(path_str)\n",
        "    candidates = [\n",
        "        path,\n",
        "        Path(path.name),\n",
        "        Path('evals') / path.name,\n",
        "        Path('..') / path,\n",
        "    ]\n",
        "    for candidate in candidates:\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "    return path\n",
        "\n",
        "\n",
        "def generate_run_name(run: Dict[str, Any]) -> str:\n",
        "    \"\"\"Generate a unique/differentiator name from run parameters or run_id.\"\"\"\n",
        "    params = run.get('params')\n",
        "    run_id = run.get('run_id', 'unknown')\n",
        "    \n",
        "    # Extract timestamp from run_id (e.g., temporal-vit-20260106-044352 -> 01/06-0443)\n",
        "    parts = run_id.split('-')\n",
        "    if len(parts) >= 4:\n",
        "        date_part = parts[-2]  # e.g., 20260106\n",
        "        time_part = parts[-1]  # e.g., 044352\n",
        "        short_ts = f\"{date_part[4:6]}/{date_part[6:8]}-{time_part[:4]}\"\n",
        "    else:\n",
        "        short_ts = run_id[-12:] if len(run_id) > 12 else run_id\n",
        "    \n",
        "    if params and isinstance(params, dict):\n",
        "        # Build name from key parameters\n",
        "        name_parts = []\n",
        "        \n",
        "        # Key differentiating parameters\n",
        "        param_abbrevs = {\n",
        "            'n_trials': 'tr',\n",
        "            'embed_dim': 'dim',\n",
        "            'n_layers': 'L',\n",
        "            'n_heads': 'H',\n",
        "            'dropout': 'do',\n",
        "            'drop_path': 'dp',\n",
        "            'lr': 'lr',\n",
        "            'weight_decay': 'wd',\n",
        "            'label_smoothing': 'ls',\n",
        "            'batch_size': 'bs',\n",
        "        }\n",
        "        \n",
        "        for key, abbrev in param_abbrevs.items():\n",
        "            if key in params:\n",
        "                val = params[key]\n",
        "                if isinstance(val, float):\n",
        "                    if val < 0.01:\n",
        "                        val_str = f\"{val:.0e}\"\n",
        "                    else:\n",
        "                        val_str = f\"{val:.2g}\"\n",
        "                else:\n",
        "                    val_str = str(val)\n",
        "                name_parts.append(f\"{abbrev}{val_str}\")\n",
        "        \n",
        "        if name_parts:\n",
        "            return f\"{short_ts} ({', '.join(name_parts[:4])})\"\n",
        "    \n",
        "    return short_ts\n",
        "\n",
        "\n",
        "def extract_metrics_df(run: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"Extract metrics from run into a DataFrame.\"\"\"\n",
        "    metrics = run.get('metrics', [])\n",
        "    if not metrics:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    df = pd.DataFrame(metrics)\n",
        "    # Filter to training epochs only (exclude test-only rows)\n",
        "    if 'val/loss' in df.columns:\n",
        "        df = df.dropna(subset=['val/loss'])\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_baselines(path_str: str = 'evals/baseline_results.json') -> List[Dict[str, Any]]:\n",
        "    \"\"\"Load baseline results from JSON file.\"\"\"\n",
        "    path = resolve_path(path_str)\n",
        "    if not path.exists():\n",
        "        print(f\"Baseline results not found at {path}. Run collect_baseline_results.py first.\")\n",
        "        return []\n",
        "    \n",
        "    with open(path, 'r', encoding='utf-8') as handle:\n",
        "        payload = json.load(handle)\n",
        "    return payload.get('baselines', [])\n",
        "\n",
        "\n",
        "# Load run details\n",
        "run_details_path = resolve_path('evals/run_details.json')\n",
        "if not run_details_path.exists():\n",
        "    run_details_path = resolve_path('run_details.json')\n",
        "\n",
        "with open(run_details_path, 'r', encoding='utf-8') as handle:\n",
        "    payload = json.load(handle)\n",
        "\n",
        "runs = payload.get('runs', [])\n",
        "if not runs:\n",
        "    raise ValueError('No runs found in run_details.json')\n",
        "\n",
        "baselines = load_baselines()\n",
        "\n",
        "print(f\"Found {len(runs)} runs\")\n",
        "print(f\"Found {len(baselines)} baseline results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for each run\n",
        "run_data = []\n",
        "for run in runs:\n",
        "    df = extract_metrics_df(run)\n",
        "    if df.empty:\n",
        "        continue\n",
        "    \n",
        "    name = generate_run_name(run)\n",
        "    run_data.append({\n",
        "        'name': name,\n",
        "        'run_id': run.get('run_id'),\n",
        "        'df': df,\n",
        "        'summary': run.get('summary', {}),\n",
        "    })\n",
        "\n",
        "print(f\"Runs with metrics: {len(run_data)}\")\n",
        "for rd in run_data:\n",
        "    print(f\"  - {rd['name']}: {len(rd['df'])} epochs\")\n",
        "\n",
        "print('\\nBaselines:')\n",
        "for bl in baselines:\n",
        "    test_auc = bl.get('test', {}).get('auc')\n",
        "    test_acc = bl.get('test', {}).get('acc')\n",
        "    auc_str = f\"{test_auc:.4f}\" if isinstance(test_auc, float) else 'N/A'\n",
        "    acc_str = f\"{test_acc:.4f}\" if isinstance(test_acc, float) else 'N/A'\n",
        "    print(f\"  - {bl['name']}: test_auc={auc_str}, test_acc={acc_str}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 1: Loss over epochs\n",
        "fig, ax = plt.subplots(figsize=(9, 5))\n",
        "\n",
        "for rd in run_data:\n",
        "    df = rd['df']\n",
        "    name = rd['name']\n",
        "    \n",
        "    if 'val/loss' not in df.columns or 'step' not in df.columns:\n",
        "        continue\n",
        "    \n",
        "    epochs = df['step'].values\n",
        "    val_loss = df['val/loss'].values\n",
        "    \n",
        "    # Plot the line\n",
        "    line, = ax.plot(epochs, val_loss, marker='o', markersize=4, label=name)\n",
        "    \n",
        "    # Highlight best (minimum) loss\n",
        "    best_idx = val_loss.argmin()\n",
        "    best_epoch = epochs[best_idx]\n",
        "    best_val = val_loss[best_idx]\n",
        "    ax.scatter([best_epoch], [best_val], s=150, c=line.get_color(), \n",
        "               marker='*', edgecolors='black', linewidths=1, zorder=5)\n",
        "    ax.annotate(f'{best_val:.3f}', (best_epoch, best_val), \n",
        "                textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
        "\n",
        "ax.set_title('Validation Loss over Epochs', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Loss', fontsize=12)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend(loc='best', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 2: Validation AUC over epochs\n",
        "fig, ax = plt.subplots(figsize=(9, 5))\n",
        "\n",
        "for rd in run_data:\n",
        "    df = rd['df']\n",
        "    name = rd['name']\n",
        "    \n",
        "    if 'val/auc' not in df.columns or 'step' not in df.columns:\n",
        "        continue\n",
        "    \n",
        "    epochs = df['step'].values\n",
        "    val_auc = df['val/auc'].values\n",
        "    \n",
        "    # Plot the line\n",
        "    line, = ax.plot(epochs, val_auc, marker='o', markersize=4, label=name)\n",
        "    \n",
        "    # Highlight best (maximum) AUC\n",
        "    best_idx = val_auc.argmax()\n",
        "    best_epoch = epochs[best_idx]\n",
        "    best_val = val_auc[best_idx]\n",
        "    ax.scatter([best_epoch], [best_val], s=150, c=line.get_color(), \n",
        "               marker='*', edgecolors='black', linewidths=1, zorder=5)\n",
        "    ax.annotate(f'{best_val:.4f}', (best_epoch, best_val), \n",
        "                textcoords='offset points', xytext=(5, -10), fontsize=8)\n",
        "\n",
        "ax.set_title('Validation AUC over Epochs', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('AUC', fontsize=12)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend(loc='best', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 3: Validation Accuracy over epochs\n",
        "fig, ax = plt.subplots(figsize=(9, 5))\n",
        "\n",
        "for rd in run_data:\n",
        "    df = rd['df']\n",
        "    name = rd['name']\n",
        "    \n",
        "    if 'val/acc' not in df.columns or 'step' not in df.columns:\n",
        "        continue\n",
        "    \n",
        "    epochs = df['step'].values\n",
        "    val_acc = df['val/acc'].values\n",
        "    \n",
        "    # Plot the line\n",
        "    line, = ax.plot(epochs, val_acc, marker='o', markersize=4, label=name)\n",
        "    \n",
        "    # Highlight best (maximum) accuracy\n",
        "    best_idx = val_acc.argmax()\n",
        "    best_epoch = epochs[best_idx]\n",
        "    best_val = val_acc[best_idx]\n",
        "    ax.scatter([best_epoch], [best_val], s=150, c=line.get_color(), \n",
        "               marker='*', edgecolors='black', linewidths=1, zorder=5)\n",
        "    ax.annotate(f'{best_val:.3f}', (best_epoch, best_val), \n",
        "                textcoords='offset points', xytext=(5, -10), fontsize=8)\n",
        "\n",
        "ax.set_title('Validation Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Accuracy', fontsize=12)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend(loc='best', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 4: Test AUC comparison (ViT vs baselines)\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "model_names = []\n",
        "test_aucs = []\n",
        "colors = []\n",
        "\n",
        "for rd in run_data:\n",
        "    test_auc = rd['summary'].get('last_test_auc')\n",
        "    if isinstance(test_auc, float):\n",
        "        model_names.append(f\"ViT: {rd['name']}\")\n",
        "        test_aucs.append(test_auc)\n",
        "        colors.append('steelblue')\n",
        "\n",
        "baseline_color_map = {\n",
        "    'log_reg': 'coral',\n",
        "    'logistic_regression': 'coral',\n",
        "    'xgboost': 'forestgreen',\n",
        "    'random_forest': 'purple',\n",
        "}\n",
        "for bl in baselines:\n",
        "    test_auc = bl.get('test', {}).get('auc')\n",
        "    if isinstance(test_auc, float):\n",
        "        model_names.append(bl['name'])\n",
        "        test_aucs.append(test_auc)\n",
        "        colors.append(baseline_color_map.get(bl.get('model_type', ''), 'gray'))\n",
        "\n",
        "if not test_aucs:\n",
        "    print('No test AUC values available to plot.')\n",
        "else:\n",
        "    sorted_indices = sorted(range(len(test_aucs)), key=lambda i: test_aucs[i], reverse=True)\n",
        "    model_names = [model_names[i] for i in sorted_indices]\n",
        "    test_aucs = [test_aucs[i] for i in sorted_indices]\n",
        "    colors = [colors[i] for i in sorted_indices]\n",
        "\n",
        "    bars = ax.barh(range(len(model_names)), test_aucs, color=colors, edgecolor='black', linewidth=0.5)\n",
        "    for bar, auc in zip(bars, test_aucs):\n",
        "        ax.text(auc + 0.005, bar.get_y() + bar.get_height() / 2, f'{auc:.4f}',\n",
        "                va='center', ha='left', fontsize=9)\n",
        "\n",
        "    ax.set_yticks(range(len(model_names)))\n",
        "    ax.set_yticklabels(model_names, fontsize=10)\n",
        "    ax.set_xlabel('Test AUC', fontsize=12)\n",
        "    ax.set_title('Test AUC Comparison: ViT vs Baselines', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlim(0, max(test_aucs) * 1.1)\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='steelblue', edgecolor='black', label='ViT'),\n",
        "        Patch(facecolor='coral', edgecolor='black', label='Logistic Regression'),\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 5: Test Accuracy comparison (ViT vs baselines)\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "model_names = []\n",
        "test_accs = []\n",
        "colors = []\n",
        "\n",
        "for rd in run_data:\n",
        "    test_acc = rd['summary'].get('last_test_acc')\n",
        "    if isinstance(test_acc, float):\n",
        "        model_names.append(f\"ViT: {rd['name']}\")\n",
        "        test_accs.append(test_acc)\n",
        "        colors.append('steelblue')\n",
        "\n",
        "baseline_color_map = {\n",
        "    'log_reg': 'coral',\n",
        "    'logistic_regression': 'coral',\n",
        "    'xgboost': 'forestgreen',\n",
        "    'random_forest': 'purple',\n",
        "}\n",
        "for bl in baselines:\n",
        "    test_acc = bl.get('test', {}).get('acc')\n",
        "    if isinstance(test_acc, float):\n",
        "        model_names.append(bl['name'])\n",
        "        test_accs.append(test_acc)\n",
        "        colors.append(baseline_color_map.get(bl.get('model_type', ''), 'gray'))\n",
        "\n",
        "if not test_accs:\n",
        "    print('No test accuracy values available to plot.')\n",
        "else:\n",
        "    sorted_indices = sorted(range(len(test_accs)), key=lambda i: test_accs[i], reverse=True)\n",
        "    model_names = [model_names[i] for i in sorted_indices]\n",
        "    test_accs = [test_accs[i] for i in sorted_indices]\n",
        "    colors = [colors[i] for i in sorted_indices]\n",
        "\n",
        "    bars = ax.barh(range(len(model_names)), test_accs, color=colors, edgecolor='black', linewidth=0.5)\n",
        "    for bar, acc in zip(bars, test_accs):\n",
        "        ax.text(acc + 0.005, bar.get_y() + bar.get_height() / 2, f'{acc:.4f}',\n",
        "                va='center', ha='left', fontsize=9)\n",
        "\n",
        "    ax.set_yticks(range(len(model_names)))\n",
        "    ax.set_yticklabels(model_names, fontsize=10)\n",
        "    ax.set_xlabel('Test Accuracy', fontsize=12)\n",
        "    ax.set_title('Test Accuracy Comparison: ViT vs Baselines', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlim(0, max(test_accs) * 1.1)\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='steelblue', edgecolor='black', label='ViT'),\n",
        "        Patch(facecolor='coral', edgecolor='black', label='Logistic Regression'),\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary table with baselines\n",
        "summary_data = []\n",
        "\n",
        "for rd in run_data:\n",
        "    df = rd['df']\n",
        "    summary = rd['summary']\n",
        "    \n",
        "    row = {\n",
        "        'Model': 'ViT',\n",
        "        'Run': rd['name'],\n",
        "        'Epochs': len(df),\n",
        "        'Best Val AUC': summary.get('best_val_auc'),\n",
        "        'Best AUC Epoch': summary.get('best_val_auc_step'),\n",
        "        'Test AUC': summary.get('last_test_auc'),\n",
        "        'Test Acc': summary.get('last_test_acc'),\n",
        "    }\n",
        "    summary_data.append(row)\n",
        "\n",
        "for bl in baselines:\n",
        "    row = {\n",
        "        'Model': bl.get('model_type', 'baseline'),\n",
        "        'Run': bl['name'],\n",
        "        'Epochs': '-',\n",
        "        'Best Val AUC': bl.get('val', {}).get('auc'),\n",
        "        'Best AUC Epoch': '-',\n",
        "        'Test AUC': bl.get('test', {}).get('auc'),\n",
        "        'Test Acc': bl.get('test', {}).get('acc'),\n",
        "    }\n",
        "    summary_data.append(row)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df['_sort_key'] = summary_df['Test AUC'].apply(lambda x: x if isinstance(x, float) else -1)\n",
        "summary_df = summary_df.sort_values('_sort_key', ascending=False).drop(columns=['_sort_key'])\n",
        "\n",
        "display(summary_df)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
